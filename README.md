# Run Open WebUI

This repo provides docker-compose files for running Open-WebUI with the following services:

1. Open WebUI
2. Ollama
3. vLLM server with a local model
4. ComfyUI running Flux1 model

This is running on a home server with 2 A6000s and 1 3080Ti.
