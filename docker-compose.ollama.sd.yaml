services:
  ollama:
    volumes:
      - /mnt/nas/ollama_webui/ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    restart: always
    tty: true
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    ports:
      - ${OLLAMA_WEBAPI_PORT-11434}:11434
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2' ]
              capabilities:
                - gpu
    networks:
      - oi_net

  # vllm:
  #   container_name: vllm
  #   image: vllm/vllm-openai:latest
  #   pull_policy: always
  #   env_file:
  #     - .env
  #   environment:
  #     - HF_TOKEN=${HUGGING_TOKEN2}
  #   ports:
  #     - "8000:8000"
  #   ipc: host
  #   shm_size: '24gb'
  #   #       --tensor-parallel-size 2 --load-format safetensors  TheBloke/dolphin-2.7-mixtral-8x7b-AWQ
  #   command: >
  #     --host 0.0.0.0 --model meta-llama/Llama-3.2-11B-Vision --max-model-len 12000 --enforce_eager --max_num_seqs 8 --gpu-memory-utilization 0.97 
  #   volumes:
  #     - /mnt/nas/hf_vllm_models/:/root/.cache/huggingface
  #   runtime: nvidia
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: [ '2' ]
  #             capabilities:
  #               - gpu
  #   networks:
  #     - oi_net

  comfyui:
    container_name: comfyui
    image: frefrik/comfyui-flux:cu124
    restart: unless-stopped
    ports:
      - "8188:8188"
    volumes:
      - /mnt/nvm/repos/run_comfyui_flux/data:/app
    environment:
      - CLI_ARGS=
      - HF_TOKEN=${HUGGING_TOKEN2}
      - LOW_VRAM=${LOW_VRAM:-false}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [ gpu ]
    networks:
      - oi_net

  tika:
    image: apache/tika:latest-full
    restart: always
    ports:
      - "9998:9998"
    networks:
      - oi_net

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    pull_policy: always
    restart: always
    volumes:
      - /mnt/nas/ollama_webui/webui:/app/backend/data
    depends_on:
      - ollama
      - comfyui
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=""
      - CONTENT_EXTRACTION_ENGINE=tika
      - ENABLE_IMAGE_GENERATION=True
      - IMAGE_GENERATION_ENGINE=comfyui
      - COMFYUI_BASE_URL=http://comfyui:8188
      - IMAGE_SIZE=1024x1024
      - IMAGE_STEPS=20
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=EMPTY
    networks:
      - oi_net

networks:
  oi_net:
    driver: bridge
